# СМПР
## Постановка задачи
Имеется множество объектов(признаковое описание) **X** и и множество ответов **Y**. Задача состоит в том, чтобы построить алгоритм, который для произвольного объекта из **X** поставит в соответствие ответ из **Y**.
Для построения алгоритмов классификации опираются на гипотезу компактности.
## Гипотеза компактности
Схожим объектам соответствуют схожие ответы.
## Метрические методы
Метрические методы - класс алгоритмов которые строятся на основе метрик. Мы рассмотрим следующие:
 + [1NN]
 + [KNN]
 + [KWNN]
 + [Метод парзеновского окна]
 + [Метод потенциалов]
 
Кроме самих методов, также мы оценим их точность с помощью метода LOO(leave-one-out).

## Метод ближайшего соседа
Рассмотрим работу метода на примере выборки ирисов Фишера на двух признаках - длине и ширине лепестков.
Сначала отобразим выборку на плоскости. Для этого создадим вектор цветов, координаты которого будут соответствовать разным видам цветов.
Далее, с помощью функции plot(), отобразим все объекты на плоскости.  
<img src="https://github.com/VivaHades/SMPR/blob/main/XL.png" />

Рассмотрим алгоритм.
Для реализации нам понадобятся две функции - функция вычисления евклидова расстояния, и функция-классификатор - алгоритм  1NN.
```r
euclDistance = function(u,v){     #Евклидово расстояние
  sqrt(sum((u-v)^2))
}

NN = function(xl,z){   #xl-выборка, z - классифицируемая точка
  l = dim(xl)[1]       #число элементов в выборке
  n = dim(xl)[2] - 1   #число признаков объекта
  
  distances = matrix(NA, l, 1)  #массив расстояний
  
  for(i in 1:l){
    distances[i, ] = euclDistance(xl[i, 1:n], z)
  }
  distances = cbind(xl, distances)       #связываем по столбцам массив расстояний и выборку
  min = which.min(distances[, 4])        #ищем и вызвращаем минимальный элемент
  return (distances[min, 3])
}
```
## Метод k ближайших соседей

Перейдем к рассмотрению алгоритма KNN.

``` r
kNN = function(xl,z,k){                   #k - число соседей 
  orderedXl = sortObjectsByDist(xl, z)    #сортируем по расстоянию
  n = dim(orderedXl)[2] - 1
  classes = orderedXl[1:k, n+1]           #массив k-ближайших соседей
  counts = table(classes)                 #вычисляем количество вхождений каждого класса среди k ближайших
  class = names(which.max(counts))        #вычисляем класс, который вошел в k ближайших больше всего раз
  return (class)                          
}
```

Метод можно оптимизировать - не сортировать всю выборку, а  пробежать ее k раз, находя новый минимальный, старый при этом убирать и сразу добавлять в массив classes.

## Метод k взвешенных ближайших соседей
Данный метод - модификация KNN, в которой добавляется весовая функция. Теперь, прежде чем классифицировать объект, мы будем проверять элементы какого класса имеют больший вес. В качестве весовой функции выбрана **q^i**, где q - еще один параметр, который можно подбирать,а i - индекс элемента из k ближайших.
``` r
kWNN = function(xl,z,k,q){
  orderedXl = sortObjectsByDist(xl, z)
  n = dim(orderedXl)[2] - 1
  classes = orderedXl[1:k, n+1]
  counts = table(classes)
  counts[1:length(counts)] = 0 #обнуляем
  
  for (i in 1:k) {
    counts[classes[i]] <- counts[classes[i]] + q^i #вычисляем веса 
  }
  class = names(which.max(counts))
  return (class)
}
```

## Карты классификации 6NN, 6WNN и 1NN

<img src="https://github.com/VivaHades/SMPR/blob/main/6NN.png" />
<img src="https://github.com/VivaHades/SMPR/blob/main/KWNN.png" />
<img src="https://github.com/VivaHades/SMPR/blob/main/1NN.png" />

##LOO для KNN и KWNN##
Для определения оптимальных параметров существует метод LOO. Суть его заключается в следующем: берем нашу обучающую выборку. Из нее извлекаем один элемент и запускаем алгоритм для всех комбинаций параметров. Если алгоритм классифицирует объект неправильно - добавляем одну ошибку. Повторяем это для всей выборки. В результате - получаем массив, в котором содержится количество ошибок для всех вариантов параметров. Найдя минимальный элемент массива получим наилучшие параметры.

LOO для KNN
